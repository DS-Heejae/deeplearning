{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP (MNIST, Tensorflow)\n",
    "In this tutorial, we will use MNIST data to practice Multi Layer Perceptron with Tensorflow.\n",
    "\n",
    "Three concepts you will learn from this tutorial,  \n",
    "1. tensorflow MLP implementation  \n",
    "2. drop out  \n",
    "3. early stopping  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: -c: line 0: syntax error near unexpected token `('\r\n",
      "/bin/sh: -c: line 0: `[Image of Perceptron](https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/MLP_XOR.png) '\r\n"
     ]
    }
   ],
   "source": [
    "![Image of Perceptron](https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/MLP_XOR.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data has **60000** samples  \n",
    "test data has **10000** samples   \n",
    "every data is **28 * 28** pixels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: -c: line 0: syntax error near unexpected token `('\r\n",
      "/bin/sh: -c: line 0: `[Image of Perceptron](https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/mnist_sample.png) '\r\n"
     ]
    }
   ],
   "source": [
    "![Image of Perceptron](https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/mnist_sample.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train data into train and validation data\n",
    "Validation during training gives advantages below,  \n",
    "1) check if train goes well based on validation score  \n",
    "2) apply **early stopping** when validation score doesn't improve while train score goes up (overcome **overfitting**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val  = x_train[50000:60000]\n",
    "x_train = x_train[0:50000]\n",
    "y_val  = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 50000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"train data has \" + str(x_train.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_train.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation data has 10000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"validation data has \" + str(x_val.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_val.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28 * 28 pixels has gray scale value from **0** to **255**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# sample to show gray scale values\n",
    "print(x_train[0][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each train data has its label **0** to **9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1]\n"
     ]
    }
   ],
   "source": [
    "# sample to show labels for first train data to 10th train data\n",
    "print(y_train[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data has **10000** samples  \n",
    "every test data is **28 * 28** image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data has 10000 samples\n",
      "every test data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"test data has \" + str(x_test.shape[0]) + \" samples\")\n",
    "print(\"every test data is \" + str(x_test.shape[1]) \n",
    "      + \" * \" + str(x_test.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape\n",
    "In order to fully connect all pixels to hidden layer,  \n",
    "we will reshape (28, 28) into (28x28,1) shape.  \n",
    "It means we flatten row x column shape to an array having 28x28 (756) items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(50000, 784)\n",
    "x_val = x_val.reshape(10000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize data\n",
    "normalization helps deep learning speed up on optimization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_val /= gray_scale\n",
    "x_test /= gray_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label to one hot encoding value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow MLP Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x):\n",
    "    # hidden layer1\n",
    "    w1 = tf.Variable(tf.random_uniform([784,256]))\n",
    "    b1 = tf.Variable(tf.zeros([256]))\n",
    "    h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "    # hidden layer2\n",
    "    w2 = tf.Variable(tf.random_uniform([256,128]))\n",
    "    b2 = tf.Variable(tf.zeros([128]))\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "    h2_drop = tf.nn.dropout(h2, keep_prob)\n",
    "    # output layer\n",
    "    w3 = tf.Variable(tf.random_uniform([128,10]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "    logits= tf.matmul(h2_drop, w3) + b3\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, validation accuracy: 0.2212, loss: 9524.407270507818\n",
      "epoch: 1, validation accuracy: 0.6217, loss: 517.2178720712662\n",
      "epoch: 2, validation accuracy: 0.6637, loss: 2.4575348401069643\n",
      "epoch: 3, validation accuracy: 0.6909, loss: 1.6795690083503723\n",
      "epoch: 4, validation accuracy: 0.7216, loss: 1.446189036369324\n",
      "epoch: 5, validation accuracy: 0.751, loss: 1.2856665849685671\n",
      "epoch: 6, validation accuracy: 0.7795, loss: 1.1384457182884216\n",
      "epoch: 7, validation accuracy: 0.7978, loss: 1.052964658737183\n",
      "epoch: 8, validation accuracy: 0.8125, loss: 0.9662364649772648\n",
      "epoch: 9, validation accuracy: 0.8282, loss: 0.8758240914344784\n",
      "epoch: 10, validation accuracy: 0.8375, loss: 0.8203610956668854\n",
      "epoch: 11, validation accuracy: 0.8478, loss: 0.7551597130298612\n",
      "epoch: 12, validation accuracy: 0.8563, loss: 0.7112305164337158\n",
      "epoch: 13, validation accuracy: 0.862, loss: 0.659562692642212\n",
      "epoch: 14, validation accuracy: 0.8713, loss: 0.6162692201137541\n",
      "epoch: 15, validation accuracy: 0.8753, loss: 0.5917496967315675\n",
      "epoch: 16, validation accuracy: 0.883, loss: 0.5637020111083983\n",
      "epoch: 17, validation accuracy: 0.8868, loss: 0.5301771557331085\n",
      "epoch: 18, validation accuracy: 0.8914, loss: 0.5129731404781341\n",
      "epoch: 19, validation accuracy: 0.8958, loss: 0.48312083840370174\n",
      "epoch: 20, validation accuracy: 0.8979, loss: 0.4673907762765884\n",
      "epoch: 21, validation accuracy: 0.9007, loss: 0.44353654205799103\n",
      "epoch: 22, validation accuracy: 0.9043, loss: 0.4279800254106521\n",
      "epoch: 23, validation accuracy: 0.906, loss: 0.41471620202064535\n",
      "epoch: 24, validation accuracy: 0.9085, loss: 0.39740209221839906\n",
      "epoch: 25, validation accuracy: 0.9105, loss: 0.38415218651294714\n",
      "epoch: 26, validation accuracy: 0.9132, loss: 0.376016516983509\n",
      "epoch: 27, validation accuracy: 0.9156, loss: 0.3597130611538888\n",
      "epoch: 28, validation accuracy: 0.917, loss: 0.3513201761245727\n",
      "epoch: 29, validation accuracy: 0.9187, loss: 0.3401906955242157\n",
      "epoch: 30, validation accuracy: 0.9197, loss: 0.32798196136951446\n",
      "epoch: 31, validation accuracy: 0.9215, loss: 0.3197318279743194\n",
      "epoch: 32, validation accuracy: 0.921, loss: 0.3114835977554322\n",
      "epoch: 33, validation accuracy: 0.9234, loss: 0.3074904572963714\n",
      "epoch: 34, validation accuracy: 0.9235, loss: 0.2953260946273804\n",
      "epoch: 35, validation accuracy: 0.9243, loss: 0.2921421691775321\n",
      "epoch: 36, validation accuracy: 0.9257, loss: 0.2788288873434067\n",
      "epoch: 37, validation accuracy: 0.928, loss: 0.272612279355526\n",
      "epoch: 38, validation accuracy: 0.9285, loss: 0.2685207036137581\n",
      "epoch: 39, validation accuracy: 0.9295, loss: 0.2619758325815201\n",
      "epoch: 40, validation accuracy: 0.9311, loss: 0.2578659203648568\n",
      "epoch: 41, validation accuracy: 0.9302, loss: 0.25156045526266096\n",
      "epoch: 42, validation accuracy: 0.9308, loss: 0.2461509039998055\n",
      "epoch: 43, validation accuracy: 0.9322, loss: 0.2415402254462242\n",
      "epoch: 44, validation accuracy: 0.9329, loss: 0.23282349675893785\n",
      "epoch: 45, validation accuracy: 0.9328, loss: 0.2324955299496651\n",
      "epoch: 46, validation accuracy: 0.9342, loss: 0.22557173311710357\n",
      "epoch: 47, validation accuracy: 0.9339, loss: 0.2217369768023491\n",
      "epoch: 48, validation accuracy: 0.9358, loss: 0.21656990647315988\n",
      "epoch: 49, validation accuracy: 0.9362, loss: 0.21325474739074707\n",
      "epoch: 50, validation accuracy: 0.9377, loss: 0.21012882858514784\n",
      "epoch: 51, validation accuracy: 0.9374, loss: 0.20572267770767214\n",
      "early stopped on 51\n",
      "[Test Accuracy] : 0.9319\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    max_val_acc = 0.0\n",
    "    for epoch in range(100):\n",
    "        avg_loss = 0.\n",
    "        start = 0; end = 1000\n",
    "        for i in range(50):\n",
    "            _, loss = sess.run([train_op, loss_op], \n",
    "                               feed_dict={x: x_train[start: end], y: y_train[start: end], \n",
    "                                          keep_prob: 0.9})\n",
    "            start += 1000\n",
    "            end += 1000\n",
    "            # Compute average loss\n",
    "            avg_loss += loss / 50\n",
    "            \n",
    "        # Validate model\n",
    "        preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        cur_val_acc = accuracy.eval({x: x_val, y: y_val, keep_prob: 1.0})\n",
    "        print(\"epoch: \"+str(epoch)+\", validation accuracy: \" \n",
    "              + str(cur_val_acc) +', loss: '+str(avg_loss))\n",
    "        if epoch > 50 and cur_val_acc < max_val_acc + 0.0005:\n",
    "            print(\"early stopped on \"+str(epoch))\n",
    "            break\n",
    "        else:\n",
    "            if max_val_acc < cur_val_acc:\n",
    "                max_val_acc = cur_val_acc\n",
    "    \n",
    "    # Test model\n",
    "    preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"[Test Accuracy] :\", accuracy.eval({x: x_test, y: y_test, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
