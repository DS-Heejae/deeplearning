{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP XOR Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Layer Perceptron (a.k.a MLP) which is basic model of deep learning works like below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Perceptron](https://www.ritchievink.com/img/post-9-mlp/nn_diagram_1.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuron (a.k.a node, perceptron) works like below picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Perceptron](https://i.stack.imgur.com/VqOpE.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various activation functions used by many type of deep learnings,  \n",
    "Traditionally, Single Neuron Perceptron used step function as activation function.  \n",
    "Single Perceptron could solve AND, OR operation, but it couldn't solve XOR operation.\n",
    "XOR can be solved by Multi Layer Perceptron (MLP), even the MLP has perceptron in its name,  \n",
    "any activation function can be used for MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Perceptron](https://qph.ec.quoracdn.net/main-qimg-01c26eabd976b027e49015428b7fcf01?convert_to_webp=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## single perceptron only works on linearly separable classification\n",
    "One perceptron is one decision boundary, so it only solve linearly separable problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Perceptron](https://qph.fs.quoracdn.net/main-qimg-a6c557af4280d1f85cacc66e048e82f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP (multi layer perceptron) with two neurons in hidden layer can solve XOR.  \n",
    "Two neurons in hidden layer will draw two boundary lines (z1, z2), \n",
    "\n",
    "we can make z1, z2 truth table like below,\n",
    "z1, z2, value\n",
    "0,  0,  0\n",
    "0,  1,  1\n",
    "1,  0,  1\n",
    "\n",
    "As you can see from below upper 2d chart, now it is linearly separable on z1, z2 axis,  \n",
    "one perceptron in the next layer can classify output from hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Perceptron](http://cps0715.weebly.com/uploads/7/4/0/3/74035485/8009014_orig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Tensorflow Graph\n",
    "firstly, we will define train data shape.  \n",
    "XOR train data has input X and output Y.  \n",
    "\n",
    "X is [4,2] shape like below,  \n",
    "[0, 0], [0, 1], [1, 0], [1, 1]  \n",
    "\n",
    "Y is [4,1] shape like below,  \n",
    "[[0], [1], [1], [0]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[4,2])\n",
    "Y = tf.placeholder(tf.float32, shape=[4,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define first layer has two neurons taking two input values.  \n",
    "W1 = tf.Variable(tf.random_uniform([2,2]))\n",
    "# each neuron has one bias.\n",
    "B1 = tf.Variable(tf.zeros([2]))\n",
    "# First Layer's output is Z which is the sigmoid(W1 * X + B1)\n",
    "Z = tf.sigmoid(tf.matmul(X, W1) + B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define second layer has one neurons taking two input values.  \n",
    "W2 = tf.Variable(tf.random_uniform([2,1]))\n",
    "# one neuron has one bias.\n",
    "B2 = tf.Variable(tf.zeros([1]))\n",
    "# Second Layer's output is Y_hat which is the sigmoid(W2 * Z + B2)\n",
    "Y_hat = tf.sigmoid(tf.matmul(Z, W2) + B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross entropy\n",
    "loss = tf.reduce_mean(-1*((Y*tf.log(Y_hat))+((1-Y)*tf.log(1.0-Y_hat))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_X = [[0,0],[0,1],[1,0],[1,1]]\n",
    "train_Y = [[0],[1],[1],[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data: [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
      "Epoch :  0\n",
      "Bias1  [-0.0015105  -0.00148694]\n",
      "Output :  [[0.6631402 ]\n",
      " [0.6786914 ]\n",
      " [0.71121323]\n",
      " [0.722656  ]]\n",
      "Epoch :  5000\n",
      "Bias1  [-0.03477474 -0.30622375]\n",
      "Output :  [[0.2932655 ]\n",
      " [0.5733818 ]\n",
      " [0.5659242 ]\n",
      " [0.60077167]]\n",
      "Epoch :  10000\n",
      "Bias1  [-4.524982  -2.0431015]\n",
      "Output :  [[0.08664235]\n",
      " [0.89662325]\n",
      " [0.89681584]\n",
      " [0.13940719]]\n",
      "Epoch :  15000\n",
      "Bias1  [-6.298708 -2.640406]\n",
      "Output :  [[0.03407335]\n",
      " [0.97077954]\n",
      " [0.9708251 ]\n",
      " [0.033969  ]]\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    print(\"train data: \"+str(train_X))\n",
    "    for i in range(20000):\n",
    "        sess.run(train_step, feed_dict={X: train_X, Y: train_Y})\n",
    "        if i % 5000 == 0:\n",
    "            print('Epoch : ', i)\n",
    "            print('Output : ', sess.run(Y_hat, feed_dict={X: train_X, Y: train_Y}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
