{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP (MNIST, Tensorflow) with Early Stopping\n",
    "In this tutorial, we will apply early stopping on MNIST MLP tensorflow code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Architecture\n",
    "here is the overview of MLP architecture we will implement with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data has **60000** samples  \n",
    "test data has **10000** samples   \n",
    "every data is **28 * 28** pixels  \n",
    "\n",
    "below image shows 28*28 pixel image sample for hand written number '0' from MNIST data.  \n",
    "MNIST is gray scale image [0 to 255] for hand written number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![0 from MNIST](https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/mnist_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train data into train and validation data\n",
    "Validation during training gives advantages below,  \n",
    "1) check if train goes well based on validation score  \n",
    "2) apply **early stopping** when validation score doesn't improve while train score goes up (overcome **overfitting**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val  = x_train[50000:60000]\n",
    "x_train = x_train[0:50000]\n",
    "y_val  = y_train[50000:60000]\n",
    "y_train = y_train[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 50000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"train data has \" + str(x_train.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_train.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation data has 10000 samples\n",
      "every train data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"validation data has \" + str(x_val.shape[0]) + \" samples\")\n",
    "print(\"every train data is \" + str(x_val.shape[1]) \n",
    "      + \" * \" + str(x_train.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28 * 28 pixels has gray scale value from **0** to **255**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# sample to show gray scale values\n",
    "print(x_train[0][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each train data has its label **0** to **9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1]\n"
     ]
    }
   ],
   "source": [
    "# sample to show labels for first train data to 10th train data\n",
    "print(y_train[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data has **10000** samples  \n",
    "every test data is **28 * 28** image  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data has 10000 samples\n",
      "every test data is 28 * 28 image\n"
     ]
    }
   ],
   "source": [
    "print(\"test data has \" + str(x_test.shape[0]) + \" samples\")\n",
    "print(\"every test data is \" + str(x_test.shape[1]) \n",
    "      + \" * \" + str(x_test.shape[2]) + \" image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape\n",
    "In order to fully connect all pixels to hidden layer,  \n",
    "we will reshape (28, 28) into (28x28,1) shape.  \n",
    "It means we flatten row x column shape to an array having 28x28 (756) items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/reshape_mnist.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/reshape_mnist.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(50000, 784)\n",
    "x_val = x_val.reshape(10000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize data\n",
    "normalization usually helps faster learning speed, better performance  \n",
    "by reducing variance and giving same range to all input features.  \n",
    "since MNIST data set all input has 0 to 255, normalization only helps reducing variances.  \n",
    "it turned out normalization is better than standardization for MNIST data with my MLP architeture,    \n",
    "I believe this is because relu handles 0 differently on both feed forward and back propagation.  \n",
    "handling 0 differently is important for MNIST, since 1-255 means there is some hand written,  \n",
    "while 0 means no hand written on that pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "gray_scale = 255\n",
    "x_train /= gray_scale\n",
    "x_val /= gray_scale\n",
    "x_test /= gray_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# label to one hot encoding value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow MLP Graph\n",
    "Let's implement the MLP graph with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\" width=\"500\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"https://raw.githubusercontent.com/minsuk-heo/deeplearning/master/img/simple_mlp_mnist.png\", width=500, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x):\n",
    "    # hidden layer1\n",
    "    w1 = tf.Variable(tf.random_uniform([784,256]))\n",
    "    b1 = tf.Variable(tf.zeros([256]))\n",
    "    h1 = tf.nn.relu(tf.matmul(x, w1) + b1)\n",
    "    # hidden layer2\n",
    "    w2 = tf.Variable(tf.random_uniform([256,128]))\n",
    "    b2 = tf.Variable(tf.zeros([128]))\n",
    "    h2 = tf.nn.relu(tf.matmul(h1, w2) + b2)\n",
    "    # output layer\n",
    "    w3 = tf.Variable(tf.random_uniform([128,10]))\n",
    "    b3 = tf.Variable(tf.zeros([10]))\n",
    "    logits= tf.matmul(h2, w3) + b3\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# train hyperparameters\n",
    "epoch_cnt = 300\n",
    "batch_size = 1000\n",
    "iteration = len(x_train) // batch_size\n",
    "\n",
    "earlystop_threshold = 5\n",
    "earlystop_cnt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Early Stopping\n",
    "early stopping..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train acc: 0.10196, val acc: 0.104\n",
      "epoch: 1, train acc: 0.80264, val acc: 0.8265\n",
      "epoch: 2, train acc: 0.85864, val acc: 0.8725\n",
      "epoch: 3, train acc: 0.875, val acc: 0.8851\n",
      "epoch: 4, train acc: 0.88222, val acc: 0.8886\n",
      "epoch: 5, train acc: 0.88098, val acc: 0.8867\n",
      "epoch: 6, train acc: 0.89326, val acc: 0.8979\n",
      "epoch: 7, train acc: 0.88704, val acc: 0.8929\n",
      "epoch: 8, train acc: 0.87078, val acc: 0.8799\n",
      "epoch: 9, train acc: 0.86422, val acc: 0.8722\n",
      "epoch: 10, train acc: 0.89574, val acc: 0.9013\n",
      "epoch: 11, train acc: 0.908, val acc: 0.912\n",
      "epoch: 12, train acc: 0.91702, val acc: 0.9152\n",
      "epoch: 13, train acc: 0.90424, val acc: 0.9073\n",
      "epoch: 14, train acc: 0.84018, val acc: 0.8366\n",
      "epoch: 15, train acc: 0.90604, val acc: 0.9072\n",
      "overfitting warning: 0\n",
      "epoch: 16, train acc: 0.86436, val acc: 0.8713\n",
      "epoch: 17, train acc: 0.90744, val acc: 0.9088\n",
      "overfitting warning: 0\n",
      "epoch: 18, train acc: 0.90666, val acc: 0.9114\n",
      "epoch: 19, train acc: 0.92112, val acc: 0.9214\n",
      "epoch: 20, train acc: 0.92454, val acc: 0.9215\n",
      "epoch: 21, train acc: 0.91752, val acc: 0.9173\n",
      "epoch: 22, train acc: 0.89534, val acc: 0.8965\n",
      "epoch: 23, train acc: 0.90698, val acc: 0.9053\n",
      "overfitting warning: 0\n",
      "epoch: 24, train acc: 0.91692, val acc: 0.9135\n",
      "overfitting warning: 1\n",
      "epoch: 25, train acc: 0.8586, val acc: 0.8652\n",
      "epoch: 26, train acc: 0.91392, val acc: 0.9128\n",
      "overfitting warning: 0\n",
      "epoch: 27, train acc: 0.90936, val acc: 0.9073\n",
      "epoch: 28, train acc: 0.91372, val acc: 0.909\n",
      "overfitting warning: 0\n",
      "epoch: 29, train acc: 0.92728, val acc: 0.9201\n",
      "overfitting warning: 1\n",
      "epoch: 30, train acc: 0.92124, val acc: 0.9166\n",
      "epoch: 31, train acc: 0.92154, val acc: 0.9129\n",
      "overfitting warning: 0\n",
      "epoch: 32, train acc: 0.91658, val acc: 0.9083\n",
      "epoch: 33, train acc: 0.9189, val acc: 0.9114\n",
      "overfitting warning: 0\n",
      "epoch: 34, train acc: 0.92254, val acc: 0.9139\n",
      "overfitting warning: 1\n",
      "epoch: 35, train acc: 0.90266, val acc: 0.8946\n",
      "epoch: 36, train acc: 0.93194, val acc: 0.9249\n",
      "epoch: 37, train acc: 0.91932, val acc: 0.9109\n",
      "epoch: 38, train acc: 0.93656, val acc: 0.9246\n",
      "overfitting warning: 0\n",
      "epoch: 39, train acc: 0.93046, val acc: 0.9234\n",
      "epoch: 40, train acc: 0.9172, val acc: 0.9112\n",
      "epoch: 41, train acc: 0.93886, val acc: 0.9274\n",
      "epoch: 42, train acc: 0.942, val acc: 0.9284\n",
      "epoch: 43, train acc: 0.94406, val acc: 0.9276\n",
      "overfitting warning: 0\n",
      "epoch: 44, train acc: 0.95136, val acc: 0.9335\n",
      "epoch: 45, train acc: 0.9503, val acc: 0.9291\n",
      "epoch: 46, train acc: 0.94834, val acc: 0.9313\n",
      "epoch: 47, train acc: 0.95476, val acc: 0.9332\n",
      "overfitting warning: 0\n",
      "epoch: 48, train acc: 0.96592, val acc: 0.9424\n",
      "epoch: 49, train acc: 0.96218, val acc: 0.9395\n",
      "epoch: 50, train acc: 0.96796, val acc: 0.9423\n",
      "overfitting warning: 0\n",
      "epoch: 51, train acc: 0.96146, val acc: 0.9379\n",
      "epoch: 52, train acc: 0.95942, val acc: 0.9358\n",
      "epoch: 53, train acc: 0.95758, val acc: 0.9365\n",
      "epoch: 54, train acc: 0.9569, val acc: 0.9368\n",
      "epoch: 55, train acc: 0.95206, val acc: 0.9317\n",
      "epoch: 56, train acc: 0.94388, val acc: 0.9246\n",
      "epoch: 57, train acc: 0.94432, val acc: 0.9238\n",
      "overfitting warning: 0\n",
      "epoch: 58, train acc: 0.96492, val acc: 0.9422\n",
      "overfitting warning: 1\n",
      "epoch: 59, train acc: 0.96516, val acc: 0.9404\n",
      "overfitting warning: 2\n",
      "epoch: 60, train acc: 0.9552, val acc: 0.9313\n",
      "epoch: 61, train acc: 0.96436, val acc: 0.9392\n",
      "overfitting warning: 0\n",
      "epoch: 62, train acc: 0.96772, val acc: 0.9449\n",
      "epoch: 63, train acc: 0.96914, val acc: 0.9444\n",
      "overfitting warning: 0\n",
      "epoch: 64, train acc: 0.9663, val acc: 0.9431\n",
      "epoch: 65, train acc: 0.97544, val acc: 0.9504\n",
      "epoch: 66, train acc: 0.97112, val acc: 0.9468\n",
      "epoch: 67, train acc: 0.979, val acc: 0.9515\n",
      "epoch: 68, train acc: 0.98056, val acc: 0.9531\n",
      "epoch: 69, train acc: 0.97914, val acc: 0.9531\n",
      "epoch: 70, train acc: 0.97962, val acc: 0.9549\n",
      "epoch: 71, train acc: 0.97808, val acc: 0.9536\n",
      "epoch: 72, train acc: 0.97326, val acc: 0.949\n",
      "epoch: 73, train acc: 0.97856, val acc: 0.9521\n",
      "overfitting warning: 0\n",
      "epoch: 74, train acc: 0.98846, val acc: 0.9581\n",
      "epoch: 75, train acc: 0.9869, val acc: 0.9571\n",
      "epoch: 76, train acc: 0.98292, val acc: 0.9541\n",
      "epoch: 77, train acc: 0.98862, val acc: 0.957\n",
      "overfitting warning: 0\n",
      "epoch: 78, train acc: 0.99346, val acc: 0.9582\n",
      "epoch: 79, train acc: 0.99386, val acc: 0.9593\n",
      "epoch: 80, train acc: 0.99306, val acc: 0.9589\n",
      "overfitting warning: 0\n",
      "epoch: 81, train acc: 0.99086, val acc: 0.9558\n",
      "overfitting warning: 1\n",
      "epoch: 82, train acc: 0.99246, val acc: 0.9577\n",
      "overfitting warning: 2\n",
      "epoch: 83, train acc: 0.9933, val acc: 0.9605\n",
      "epoch: 84, train acc: 0.99522, val acc: 0.9592\n",
      "overfitting warning: 0\n",
      "epoch: 85, train acc: 0.99312, val acc: 0.9575\n",
      "overfitting warning: 1\n",
      "epoch: 86, train acc: 0.99658, val acc: 0.961\n",
      "epoch: 87, train acc: 0.9982, val acc: 0.9615\n",
      "epoch: 88, train acc: 0.99602, val acc: 0.9596\n",
      "overfitting warning: 0\n",
      "epoch: 89, train acc: 0.9968, val acc: 0.9603\n",
      "overfitting warning: 1\n",
      "epoch: 90, train acc: 0.99562, val acc: 0.9588\n",
      "overfitting warning: 2\n",
      "epoch: 91, train acc: 0.9975, val acc: 0.9626\n",
      "epoch: 92, train acc: 0.99718, val acc: 0.962\n",
      "overfitting warning: 0\n",
      "epoch: 93, train acc: 0.99416, val acc: 0.9597\n",
      "overfitting warning: 1\n",
      "epoch: 94, train acc: 0.99486, val acc: 0.96\n",
      "overfitting warning: 2\n",
      "epoch: 95, train acc: 0.9936, val acc: 0.9583\n",
      "overfitting warning: 3\n",
      "epoch: 96, train acc: 0.99762, val acc: 0.9617\n",
      "overfitting warning: 4\n",
      "epoch: 97, train acc: 0.99754, val acc: 0.9622\n",
      "early stopped on 97\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    prev_train_acc = 0.0\n",
    "    max_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(epoch_cnt):\n",
    "        avg_loss = 0.\n",
    "        start = 0; end = batch_size\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            _, loss = sess.run([train_op, loss_op], \n",
    "                               feed_dict={x: x_train[start: end], y: y_train[start: end]})\n",
    "            start += batch_size; end += batch_size\n",
    "            # Compute train average loss\n",
    "            avg_loss += loss / iteration\n",
    "            \n",
    "        # Validate model\n",
    "        preds = tf.nn.softmax(logits)  # Apply softmax to logits\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        # train accuracy\n",
    "        cur_train_acc = accuracy.eval({x: x_train, y: y_train})\n",
    "        # validation accuarcy\n",
    "        cur_val_acc = accuracy.eval({x: x_val, y: y_val})\n",
    "        # validation loss\n",
    "        cur_val_loss = loss_op.eval({x: x_val, y: y_val})\n",
    "        \n",
    "        print(\"epoch: \"+str(epoch)+\n",
    "              \", train acc: \" + str(cur_train_acc) +\n",
    "              \", val acc: \" + str(cur_val_acc) )\n",
    "              #', train loss: '+str(avg_loss)+\n",
    "              #', val loss: '+str(cur_val_loss))\n",
    "        \n",
    "        if cur_val_acc < max_val_acc:\n",
    "            if cur_train_acc > prev_train_acc or cur_train_acc > 0.99:\n",
    "                if earlystop_cnt == earlystop_threshold:\n",
    "                    print(\"early stopped on \"+str(epoch))\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"overfitting warning: \"+str(earlystop_cnt))\n",
    "                    earlystop_cnt += 1\n",
    "            else:\n",
    "                earlystop_cnt = 0\n",
    "        else:\n",
    "            earlystop_cnt = 0\n",
    "            max_val_acc = cur_val_acc\n",
    "            # Save the variables to file.\n",
    "            save_path = saver.save(sess, \"model/model.ckpt\")\n",
    "        prev_train_acc = cur_train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with the best epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model.ckpt\n",
      "[Test Accuracy] : 0.9618\n"
     ]
    }
   ],
   "source": [
    "# Start testing\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"model/model.ckpt\")\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"[Test Accuracy] :\", accuracy.eval({x: x_test, y: y_test}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
